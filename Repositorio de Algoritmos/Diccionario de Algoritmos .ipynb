{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos de Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este documento se puede encontrar la mayoria de algoritmos que se usaron durante la clase de Elements of Machine learning. Estos algoritmos tambien fueron usados en la entrega final del proyecto en donde se realiza la prediccion del winrate de un juego de League of Legends. \n",
    "\n",
    "La mayoria de estos algoritmos fueron recolectados de una variedad de laboraorios de la clase de Machine Learning Models y Elements of Machine Learning. Si se necesita referencia adicional sober como aplicarlos, se puede visualizar cada uno de estos algoritmos en el proyecto anteriormente mencionado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Define and fit standardized data\n",
    "lda = LDA()\n",
    "ld = lda.fit_transform(wine_features_std, wine_labels)\n",
    "lda_df = pd.DataFrame(data = ld, \n",
    "        columns = ['LDA1', 'LDA2'])\n",
    "lda_df['Cluster'] = wine_labels\n",
    "\n",
    "# Print results of classification of training data\n",
    "print('Accuracy of LDA classifier on training set: {:.2f}'\n",
    "     .format(lda.score(wine_features_std, wine_labels)))\n",
    "# print('Accuracy of LDA classifier on test set: {:.2f}'\n",
    "#      .format(lda.score(X_test, y_test)))\n",
    "\n",
    "# Print top n observations of LDA df\n",
    "lda_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fea74a416e3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Source: https://cmdlinetips.com/2018/03/pca-example-in-python-with-scikit-learn/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Fit to features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Define number of principal components\n",
    "# Source: https://cmdlinetips.com/2018/03/pca-example-in-python-with-scikit-learn/\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Fit to features\n",
    "pc = pca.fit_transform(wine_features)\n",
    "\n",
    "# Dataframe of principal components and wine labels\n",
    "pc_df = pd.DataFrame(data = pc, \n",
    "        columns = ['PC1', 'PC2','PC3'])\n",
    "pc_df['Cluster'] = wine_labels\n",
    "pc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K - Means Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "cancer_features = cancer_df.loc[:, 'mean radius':]\n",
    "cancer_labels = cancer_df[['target']]\n",
    "-------------------------------------------------------\n",
    "# Define k-means parameters and cluster\n",
    "clusters = KMeans(n_clusters=2).fit_predict(cancer_features)\n",
    "-------------------------------------------------------\n",
    "# Plot the results as a two-dimensional scatter plot\n",
    "sns.scatterplot(cancer_features['mean radius'], cancer_features['mean smoothness'], hue=clusters) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation-maximization clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib.patches import Ellipse\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs), )\n",
    "        \n",
    "----------------------------------------------------------------------------------\n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2, vmin=0, vmax=.2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2).set_ylim(top=.2, bottom = 0, vmin=0, vmax=.2)\n",
    "        \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)\n",
    "----------------------------------------------------------------------------------\n",
    "# EM clustering\n",
    "gmm = GaussianMixture(n_components=2, covariance_type='full')\n",
    "gmm.fit(cancer_features[['mean radius', 'mean smoothness']])\n",
    "\n",
    "plot_gmm(gmm, np.array(cancer_features[['mean radius', 'mean smoothness']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-shift clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "# Compute clustering with MeanShift\n",
    "# The following bandwidth can be automatically detected using\n",
    "bandwidth = estimate_bandwidth(cancer_features, quantile=0.8)\n",
    "\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "#ms.fit(cancer_features[['mean radius', 'mean smoothness']])\n",
    "ms.fit(cancer_features)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "\n",
    "print(\"number of estimated clusters : %d\" % n_clusters_)\n",
    "----------------------------------------------------------------------------------\n",
    "# Add the centers to a dataframe\n",
    "centers_df = pd.DataFrame(cluster_centers).head()\n",
    "# centers_df.columns = ['x', 'y']\n",
    "centers_df['x'] = centers_df[0]\n",
    "centers_df['y'] = centers_df[2]\n",
    "centers_df.head()\n",
    "----------------------------------------------------------------------------------\n",
    "# Plot clusters in two dimensions and overlay the centers\n",
    "sns.scatterplot(x='mean radius', y='mean perimeter', data=cancer_features, hue=labels)\n",
    "sns.scatterplot(x='x', y='y', data=centers_df, color='green', s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "for linkage in ('ward', 'average', 'complete', 'single'):\n",
    "    clustering = AgglomerativeClustering(linkage=linkage, n_clusters=3)\n",
    "    #t0 = time()\n",
    "    clustering.fit(cancer_features[['mean radius', 'mean smoothness']])\n",
    "    #print(\"%s :\\t%.2fs\" % (linkage, time() - t0))\n",
    "\n",
    "    sns.scatterplot(x='mean radius', y='mean smoothness', data=cancer_features, hue=clustering.labels_)\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendogram Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take top 30 observations and plot dendrogram\n",
    "linkage_data = linkage(cancer_features[['mean radius', 'mean smoothness']].tail(30), method='centroid')\n",
    "scipy.cluster.hierarchy.dendrogram(linkage_data)\n",
    "plt.show()\n",
    "-------------------------------------------------------------------------\n",
    "# Show full dataset dendrogram\n",
    "linkage_data = linkage(cancer_features[['mean radius', 'mean smoothness']], method='centroid')\n",
    "scipy.cluster.hierarchy.dendrogram(linkage_data)\n",
    "plt.show()\n",
    "-------------------------------------------------------------------------\n",
    "# Now limit the number of trees allowed and re-plot\n",
    "# Truncate mode = 'lastp': The last p non-singleton clusters formed in the linkage are the only non-leaf nodes in the \n",
    "#  linkage; they correspond to rows Z[n-p-2:end] in Z. All other non-singleton clusters are contracted into leaf nodes.\n",
    "linkage_data = linkage(cancer_features[['mean radius', 'mean smoothness']], method='centroid')\n",
    "scipy.cluster.hierarchy.dendrogram(linkage_data, truncate_mode='lastp', p=30)\n",
    "plt.show()\n",
    "-------------------------------------------------------------------------\n",
    "# Now limit the number levels in the tree and re-plot\n",
    "# Truncate mode = 'level': No more than p levels of the dendrogram tree are displayed. A “level” includes all nodes with p\n",
    "#  merges from the last merge.\n",
    "linkage_data = linkage(cancer_features[['mean radius', 'mean smoothness']], method='centroid')\n",
    "scipy.cluster.hierarchy.dendrogram(linkage_data, truncate_mode='level', p=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresiones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresion Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "lr = logreg.fit(thyroid_features,thyroid_labels)\n",
    "-----------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "x_simple = thyroid_features.iloc[:, 4:]\n",
    "x_space = np.linspace(-5, 10, 100)\n",
    "def model(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "loss = model(x_space * lr_simple.coef_ + lr_simple.intercept_).ravel()\n",
    "plt.plot(x_space, loss, color='red', linewidth=3)\n",
    "plt.scatter(thyroid_features['tsh_diff'].ravel(), thyroid_labels, color='black', zorder=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresion Lineal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sm.add_constant(X_train)\n",
    "simple_model = sm.OLS(y_train,X_train[['firstInhibitor_red', 'const']])\n",
    "simple_result = simple_model.fit()\n",
    "-------------------------------------------------------\n",
    "\n",
    "X_test = sm.add_constant(X_test)\n",
    "y_pred_simple = simple_result.predict(X_test[['firstInhibitor_red', 'const']])\n",
    "print(simple_result.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresion Polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_reg = PolynomialFeatures(degree = 3)\n",
    "X_poly_train = poly_reg.fit_transform(pd.DataFrame(X_train['firstInhibitor_red']))\n",
    "X_poly_test = poly_reg.fit_transform(pd.DataFrame(X_test['firstInhibitor_red']))\n",
    "poly_result = poly_reg.fit(X_poly_train, y_train)\n",
    "poly_model = LinearRegression()\n",
    "poly_result = poly_model.fit(X_poly_train, y_train)\n",
    "y_poly_pred = poly_model.predict(X_poly_test)\n",
    "-----------------------------------------------------------------------------------\n",
    "poly_model = sm.OLS(y_train,X_poly_train)\n",
    "poly_result = poly_model.fit()\n",
    "print(poly_result.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel = 'linear')\n",
    "clf.fit(iris_featues, iris_labels)\n",
    "\n",
    "predictions = clf.predict(iris_featues)\n",
    "--------------------------------------------------------------------------------\n",
    "results = pd.DataFrame({'Labels':iris_labels, 'Predictions':predictions})\n",
    "--------------------------------------------------------------------------------\n",
    "def plotSVC(svc, param, X, y):\n",
    "    clf = svc\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,\n",
    "                edgecolor='k', s=20)\n",
    "\n",
    "    # Circle out the test data\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80, facecolors='none',\n",
    "                zorder=10, edgecolor='k')\n",
    "\n",
    "    plt.axis('tight')\n",
    "    x_min = X.iloc[:, 0].min()\n",
    "    x_max = X.iloc[:, 0].max()\n",
    "    y_min = X.iloc[:, 1].min()\n",
    "    y_max = X.iloc[:, 1].max()\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "    pre_z = svc.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "    Z = pre_z.reshape(XX.shape)\n",
    "    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],\n",
    "                linestyles=['--', '-', '--'])\n",
    "    \n",
    "    plt.pcolormesh(XX, YY, Z , cmap=plt.cm.Paired)\n",
    "    plt.title(param)\n",
    "    plt.show()\n",
    "--------------------------------------------------------------------------------\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "for kernel in kernels:\n",
    "    svc = SVC(kernel=kernel).fit(iris_featues, iris_labels)\n",
    "    plotSVC(svc, kernel, iris_featues, iris_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de Penalizacion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "rr = Ridge(alpha=0.01) # higher the alpha value, more restriction on the coefficients; low alpha > more generalization, coefficients are barely\n",
    "# restricted and in this case linear and ridge regression resembles\n",
    "rr = rr.fit(X_train, y_train)\n",
    "Ridge_train_score = rr.score(X_train,y_train)\n",
    "Ridge_test_score = rr.score(X_test, y_test)\n",
    "print (\"Ridge Regression Train Score low alpha:\", Ridge_train_score)\n",
    "print (\"Ridge Regression Test Score low alpha:\", Ridge_test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial regression features of nth degree\n",
    "poly_reg = PolynomialFeatures(degree = 3)\n",
    "X_poly_train = poly_reg.fit_transform(pd.DataFrame(X_train_final))\n",
    "X_poly_test = poly_reg.fit_transform(pd.DataFrame(X_test_final))\n",
    "poly_result = poly_reg.fit(X_train_final, y_train)\n",
    "\n",
    "# Fit linear model now polynomial features\n",
    "poly_model = LinearRegression()\n",
    "poly_result = poly_model.fit(X_poly_train, y_train)\n",
    "y_poly_pred = poly_model.predict(X_poly_test)\n",
    "--------------------------------------------------------------------------------\n",
    "# Lasso regression model with alpha = 10000 (high penalty) -- now with a lot of new variables\n",
    "lasso_poly_model_10k = Lasso(alpha=10000, fit_intercept=True)\n",
    "lasso_poly_model_10k.fit(X_poly_train, y_train)\n",
    "lasso_poly_10k_pred = lasso_poly_model_10k.predict(X_poly_test)\n",
    "print(lasso_poly_model_10k.score(X_poly_test, y_test))\n",
    "lasso_poly_model_10k.coef_\n",
    "--------------------------------------------------------------------------------\n",
    "# Plot to compare models\n",
    "sns.scatterplot(x = X_test_final['city-mpg'], y = y_test.values.ravel())\n",
    "sns.regplot(x = X_test_final['city-mpg'], y = lr_pred.ravel(), color='g')\n",
    "sns.regplot(x = X_test_final['city-mpg'], y = y_poly_pred.ravel(), color='gray', order=3)\n",
    "sns.regplot(x = X_test_final['city-mpg'], y = lasso_poly_10k_pred.ravel(), color='purple', order=3)\n",
    "--------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series de Tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Fit AR model\n",
    "model = ARIMA(y_train, order=(1, 0, 0))\n",
    "armodel_fit = model.fit(disp=0)\n",
    "print(armodel_fit.summary())\n",
    "# plot residual errors\n",
    "residuals = pd.DataFrame(armodel_fit.resid)\n",
    "residuals.plot()\n",
    "pyplot.show()\n",
    "residuals.plot(kind='kde')\n",
    "pyplot.show()\n",
    "print(residuals.describe())\n",
    "--------------------------------------------------------------------------------\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Fit ARIMA model\n",
    "model = ARIMA(y_train, order=(4, 1, 1))\n",
    "model_fit = model.fit(disp=0)\n",
    "print(model_fit.summary())\n",
    "# plot residual errors\n",
    "residuals = pd.DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "pyplot.show()\n",
    "residuals.plot(kind='kde')\n",
    "pyplot.show()\n",
    "print(residuals.describe())\n",
    "--------------------------------------------------------------------------------\n",
    "start_index = '2005-04-03 15:00:00'\n",
    "end_index = '2005-04-04 14:00:00'\n",
    "arforecast = armodel_fit.predict(start=start_index, end=end_index)\n",
    "--------------------------------------------------------------------------------\n",
    "start_index = '2005-04-03 15:00:00'\n",
    "end_index = '2005-04-04 14:00:00'\n",
    "forecast = model_fit.predict(start=start_index, end=end_index)\n",
    "--------------------------------------------------------------------------------\n",
    "# Plot of original data and predictions\n",
    "sns.lineplot(x=forecast.index, y=forecast)\n",
    "sns.lineplot(x=arforecast.index, y=arforecast)\n",
    "sns.lineplot(x=y_test.index, y=y_test['AH'])\n",
    "sns.lineplot(x=y_train.tail(24).index, y=y_train['AH'].tail(24))\n",
    "--------------------------------------------------------------------------------\n",
    "import statistics as stats\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "forecast_errors = [y_test['AH'][i]-forecast[i] for i in range(len(y_test))]\n",
    "print('Forecast Errors: %s' % forecast_errors)\n",
    "\n",
    "print('\\nMean Error:')\n",
    "print(stats.mean(forecast_errors))\n",
    "\n",
    "print('\\nAbsolute Mean Error:')\n",
    "print(mean_absolute_error(y_test['AH'], forecast))\n",
    "\n",
    "print('\\nRoot Mean Squared Error:')\n",
    "print(sqrt(mean_squared_error(y_test['AH'], forecast)))\n",
    "--------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-86b47dc62aa8>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-86b47dc62aa8>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    --------------------------------------------------------------------------------\u001b[0m\n\u001b[1;37m                                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "--------------------------------------------------------------------------------\n",
    "! pip install graphviz \n",
    "# Windows\n",
    "# !set PATH=PATH;C:\\path\\to\\anaconda\\Library\\bin\\graphviz\\\n",
    "\n",
    "# Mac\n",
    "# brew install graphviz\n",
    "\n",
    "# Linux\n",
    "# sudo apt-get install graphviz\n",
    "--------------------------------------------------------------------------------\n",
    "import graphviz \n",
    "dot_data = export_graphviz(clf, out_file=None, \n",
    "                     feature_names=feature_dummies.columns,  \n",
    "                     class_names=labels.unique(),  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph\n",
    "--------------------------------------------------------------------------------\n",
    "# Hyperparametros\n",
    "clf = DecisionTreeClassifier(max_depth=2)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "dot_data = export_graphviz(clf, out_file=None, \n",
    "                     feature_names=feature_dummies.columns,  \n",
    "                     class_names=labels.unique(),  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph\n",
    "--------------------------------------------------------------------------------\n",
    "y_pred = clf.predict(X_test)\n",
    "--------------------------------------------------------------------------------\n",
    "plot_confusion_matrix(y_true=y_test, y_pred=y_pred,\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arboles de Decision (Con Metodo GridSearch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "criterion=[\"gini\",\"entropy\"]\n",
    "max_depth=range(1,20,2)\n",
    "splitter=[\"best\",\"random\"]\n",
    "dt=DecisionTreeClassifier()\n",
    "grid_decision_tree=GridSearchCV(estimator=dt,cv=15,param_grid=dict(criterion=criterion,max_depth=max_depth,splitter=splitter))\n",
    "--------------------------------------------------------------------------------\n",
    "dt2=DecisionTreeClassifier(criterion=\"entropy\",max_depth=7, splitter=\"best\")\n",
    "dt2.fit(X_train,y_train)\n",
    "print(\"Score:\", dt2.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "--------------------------------------------------------------------------------\n",
    "clf.estimators_[0:5]\n",
    "--------------------------------------------------------------------------------\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from math import sqrt\n",
    "tree_predictions = clf.estimators_[15].predict(X_test)\n",
    "r2_score(y_test, tree_predictions)\n",
    "--------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOB Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = RandomForestRegressor(n_estimators=100, \n",
    "                            oob_score=True, \n",
    "                            n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "clf.oob_score_, r2_score(y_test, pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "--------------------------------------------------------------------------------\n",
    "# Define model parameters\n",
    "adaboost_reg = AdaBoostRegressor(n_estimators=100, learning_rate=1, loss='linear')\n",
    "--------------------------------------------------------------------------------\n",
    "# Train model\n",
    "adaboost_reg.fit(X_train, y_train)\n",
    "--------------------------------------------------------------------------------\n",
    "# Find train and test predictions\n",
    "prediction_train = adaboost_reg.score(X_train,y_train)\n",
    "prediction_test = adaboost_reg.score(X_test,y_test)\n",
    "print(prediction_train)\n",
    "print(prediction_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://xgboost.readthedocs.io/en/latest/python/python_intro.html\n",
    "import xgboost as xgb\n",
    "--------------------------------------------------------------------------------\n",
    "# Add data to XGBoost data matrices\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "--------------------------------------------------------------------------------\n",
    "param = {'booster': 'gbtree', 'max_depth': 5, 'eta': 0.8, 'gamma': .01} #Classification objective:binary:logistic, 'objective': 'reg:squarederror' by default \n",
    "--------------------------------------------------------------------------------\n",
    "# Train model\n",
    "num_round = 100\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "--------------------------------------------------------------------------------\n",
    "# Get model predictions\n",
    "y_train_pred = bst.predict(dtrain)\n",
    "y_test_pred = bst.predict(dtest)\n",
    "--------------------------------------------------------------------------------\n",
    "# Find train and test r^2 values\n",
    "print(r2_score(y_train, y_train_pred))\n",
    "print(r2_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN (Convolusional neural Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_calculator(number_of_images, number_of_columns):\n",
    "    if number_of_images % number_of_columns != 0:\n",
    "        return (number_of_images / number_of_columns)+1\n",
    "    else:\n",
    "        return (number_of_images / number_of_columns)\n",
    "    \n",
    "def display_image(x, img_size, number_of_images):\n",
    "    plt.figure(figsize = (8, 7))\n",
    "    if x.shape[0] > 0:\n",
    "        n_samples = x.shape[0]\n",
    "        x = x.reshape(n_samples, img_size, img_size)\n",
    "        number_of_rows = row_calculator(number_of_images, 4)\n",
    "        for i in range(number_of_images):\n",
    "            plt.subplot(number_of_rows, 4, i+1)\n",
    "            plt.imshow(x[i])\n",
    "            \n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(label_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                label_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array[i], true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')\n",
    "    \n",
    "--------------------------------------------------------------------------------\n",
    "data_train = pd.read_csv(\"csvTrainImages 13440x1024.csv\" , header = None).values\n",
    "data_label = pd.read_csv(\"csvTrainLabel 13440x1.csv\" , header = None)\n",
    "data_label = data_label.values.astype('int32')-1\n",
    "data_label = to_categorical(data_label , 28)\n",
    "data_test = pd.read_csv(\"csvTestImages 3360x1024.csv\" , header = None).values\n",
    "data_label_test = pd.read_csv(\"csvTestLabel 3360x1.csv\" , header = None)\n",
    "data_label_test = data_label_test.values.astype('int32')-1\n",
    "data_label_test = to_categorical(data_label_test , 28)\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_label.shape)\n",
    "print(data_test.shape)\n",
    "print(data_label_test.shape)\n",
    "\n",
    "#normalize \n",
    "data_train = data_train/255\n",
    "data_test = data_test/255\n",
    "data_train = np.reshape(data_train , (13440 , 32 , 32 , 1))\n",
    "data_test = np.reshape(data_test , (3360 , 32 , 32 , 1))\n",
    "input_size = data_train.shape[1:]\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "cnn_model = keras.Sequential()\n",
    "cnn_model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_size))\n",
    "cnn_model.add(Conv2D(64 , kernel_size = (5 , 5) , activation = 'relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(128 , activation = 'relu'))\n",
    "cnn_model.add(Dense(28 , activation = 'softmax'))\n",
    "--------------------------------------------------------------------------------\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer= 'adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "--------------------------------------------------------------------------------\n",
    "cnn_model_history = model.fit(data_train, data_label,\n",
    "                    batch_size=256,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(data_test, data_label_test))\n",
    "--------------------------------------------------------------------------------\n",
    "results = model.evaluate(data_test, data_label_test, verbose=0)\n",
    "print('Test loss:', results[0])\n",
    "print('Test accuracy:', results[1]*100)\n",
    "--------------------------------------------------------------------------------\n",
    "print(cnn_model_history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(cnn_model_history.history['accuracy'])\n",
    "plt.plot(cnn_model_history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['cnn_train', 'cnn_validation', 'first_train', 'first_validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(cnn_model_history.history['loss'])\n",
    "plt.plot(cnn_model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['cnn_train', 'cnn_validation', 'first_train', 'first_validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
